#### **深層学習におけるランダムシードを固定する理由**
>深層学習モデルの訓練には多くのランダムな要素が関与するため。

- モデルの重みの初期値
- 訓練データのシャッフル
- ドロップアウト: 一定の確率でノードを無効化する手法

```python
# ex)
# 以下は、PyTorchでランダムシードを42に設定する例。

import torch

torch.manual_seed(42)

# これ以降のPyTorchのランダムな動作（例：重みの初期化やデータのシャッフル）は、シード`42`を基に行れる。
# このように設定しておくと、ランダムな動作も同じ条件下で再現することが可能になる。
```

#### **注意点**
>シードを固定することで、モデルの訓練や評価の結果が再現可能になりますが、異なるシード値での訓練を複数回行い、その平均や分布を評価することも一般的。

#### **torch.nn.Module**

PyTochでのニューラルネットワーク(: 多重パーセプトロン)の構築モジュールは以下の二つ。

| 種類 | 内容 |
| --- | --- |
| [Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module) | Base class for all neural network modules. |
| [Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential) | A sequential container. |


**基本的な書き方**
- `torch.nn.Module`のサブクラスを定義してモデルを構築
- `__init__()`で使用するモジュール（レイヤー）のインスタンスを生成。
- `forward()`で所望の順番で適用。
- `super().__init__()`を忘れないように注意。

```Python
class Net(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1= nn.Linear(1000, 100)
        self.fc2= nn.Linear(100, 10)
        self.relu= nn.ReLU()
        self.dropout= nn.Dropout(0.2)

    defforward(self, x):
        x= self.fc1(x)
        x= self.relu(x)
        x= self.dropout(x)
        x= self.fc2(x)
        return x
```

**パラメータの引数指定する場合**
```
class NetParam(nn.Module):
    def __init__(self, n_input, n_hidden, n_output, p_dropout):
        super().__init__()
        self.fc1= nn.Linear(n_input, n_hidden)
        self.fc2= nn.Linear(n_hidden, n_output)
        self.relu= nn.ReLU()
        self.dropout= nn.Dropout(p_dropout)

    defforward(self, x):
        x= self.relu(self.fc1(x))
        x= self.dropout(x)
        x= self.fc2(x)
        return x
```

**クラス(: モデル名).{レイヤー名}で取得**
```Python
print(net.fc1)
# Linear(in_features=1000, out_features=100, bias=True)
```

##### **参照**
[toch.nn - PyTorch 2.1 documentation](https://pytorch.org/docs/stable/generated/torch.nn.Module.html)